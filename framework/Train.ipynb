{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51abef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from siamese import TripletBrainDataset, MRI3DAugmentation, triplet_loss\n",
    "from resnet3d import generate_model as generate_resnet\n",
    "from unet import generate_model as generate_unet\n",
    "from densenet import generate_model as generate_densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 2000\n",
    "EMBEDDING_DIM = 128\n",
    "MARGIN = 0.1\n",
    "MODEL = 'unet'\n",
    "ACCUMULATION_STEPS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'densenet':\n",
    "    BATCH_SIZE = 64\n",
    "    ACCUMULATION_STEPS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881713dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = MRI3DAugmentation(p=0.5)\n",
    "\n",
    "dataset = 'hcpaspecttriplet'\n",
    "t = 1\n",
    "DATA_PATH = f'../../../data/{dataset}'\n",
    "    \n",
    "augmentation = MRI3DAugmentation(p=0.5)  # Assuming this is defined elsewhere\n",
    "\n",
    "train_dataset = TripletBrainDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    split='train',\n",
    "    mining_strategy='hard',\n",
    "    num_negatives_per_positive=3,\n",
    "    transform=augmentation,\n",
    "    margin=MARGIN\n",
    ")\n",
    "    \n",
    "val_dataset = TripletBrainDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    split='val',\n",
    "    mining_strategy='hard',\n",
    "    num_negatives_per_positive=3,\n",
    "    margin=MARGIN\n",
    ")\n",
    "\n",
    "test_dataset = TripletBrainDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    split='test',\n",
    "    mining_strategy='random',\n",
    "    num_negatives_per_positive=3,\n",
    "    margin=MARGIN\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"Train: {len(train_dataset.twin_pairs)} twin pairs, {len(train_dataset.all_subjects)} total subjects\")\n",
    "print(f\"Val: {len(val_dataset.twin_pairs)} twin pairs, {len(val_dataset.all_subjects)} total subjects\")\n",
    "print(f\"Test: {len(test_dataset.twin_pairs)} twin pairs, {len(test_dataset.all_subjects)} total subjects\")\n",
    "print(f\"Train dataset size: {len(train_dataset)} triplets\")\n",
    "\n",
    "# Create data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Example: Test loading a batch\n",
    "print(\"\\nTesting data loading...\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch shapes:\")\n",
    "print(f\"  Anchor: {batch['anchor'].shape}\")\n",
    "print(f\"  Positive: {batch['positive'].shape}\")\n",
    "print(f\"  Negative: {batch['negative'].shape}\")\n",
    "print(f\"  Sample anchor ID: {batch['anchor_id'][0]}\")\n",
    "print(f\"  Sample positive ID: {batch['positive_id'][0]}\")\n",
    "print(f\"  Sample negative ID: {batch['negative_id'][0]}\")\n",
    "\n",
    "# Training loop example (without actual model training)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding_layers(model):\n",
    "    \"\"\"Initialize final layers with smaller weights for stable embeddings.\"\"\"\n",
    "    # Smaller initialization for embedding layers\n",
    "    nn.init.xavier_uniform_(model.fc1.weight, gain=0.5)\n",
    "    nn.init.xavier_uniform_(model.fc2.weight, gain=0.5)\n",
    "    \n",
    "    # Initialize biases to zero if they exist\n",
    "    if hasattr(model.fc1, 'bias') and model.fc1.bias is not None:\n",
    "        nn.init.zeros_(model.fc1.bias)\n",
    "    if hasattr(model.fc2, 'bias') and model.fc2.bias is not None:\n",
    "        nn.init.zeros_(model.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model = None\n",
    "\n",
    "if MODEL == 'resnet':\n",
    "    generate_model = generate_resnet\n",
    "elif MODEL == 'unet':\n",
    "    generate_model = generate_unet\n",
    "elif MODEL == 'densenet':\n",
    "    generate_model = generate_densenet\n",
    "\n",
    "model = generate_model(model_depth=10, embedding_dim=EMBEDDING_DIM, use_attention=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Clear cache more aggressively\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Use memory mapping for large datasets\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec820ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"Parameter {name} does not require grad!\")\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95de539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_regularization_loss(embeddings, lambda_reg=0.01):\n",
    "    \"\"\"Prevent embedding collapse by encouraging diversity.\"\"\"\n",
    "    # Compute pairwise distances\n",
    "    pdist = F.pdist(embeddings, p=2)\n",
    "    \n",
    "    # Penalize very small distances (collapse)\n",
    "    collapse_penalty = torch.exp(-pdist).mean()\n",
    "    \n",
    "    return lambda_reg * collapse_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet(model, dataloader, optimizer, scheduler, device, epoch, margin=1.0, \n",
    "                          scaler=None, accumulation_steps=1, use_regularization=True):\n",
    "    \"\"\"\n",
    "    Training loop with proper scheduler placement for OneCycleLR.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_pos_dist = 0\n",
    "    total_neg_dist = 0\n",
    "    total_active_ratio = 0\n",
    "    gradient_errors = 0\n",
    "    use_mixed_precision = scaler is not None\n",
    "\n",
    "    mining_strategy = 'hard'\n",
    "    \n",
    "    # Update mining strategy\n",
    "    if hasattr(dataloader.dataset, 'mining_strategy'):\n",
    "        dataloader.dataset.mining_strategy = mining_strategy\n",
    "        \n",
    "        if mining_strategy in ['hard', 'semi_hard']:\n",
    "            dataloader.dataset.set_model_for_mining(model)\n",
    "            if epoch % 3 == 0:\n",
    "                dataloader.dataset.clear_embedding_cache()\n",
    "    \n",
    "    num_batches = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), \n",
    "                total=len(dataloader),\n",
    "                desc=f'Epoch {epoch:3d} [{mining_strategy:9s}]',\n",
    "                leave=False,\n",
    "                ncols=120)\n",
    "\n",
    "    for batch_idx, batch in pbar:\n",
    "        try:\n",
    "            anchor = batch['anchor'].to(device, non_blocking=True)\n",
    "            positive = batch['positive'].to(device, non_blocking=True)\n",
    "            negative = batch['negative'].to(device, non_blocking=True)\n",
    "            \n",
    "            if scaler is not None:\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    anchor_emb = model(anchor)\n",
    "                    positive_emb = model(positive)\n",
    "                    negative_emb = model(negative)\n",
    "                    \n",
    "                    loss, pos_dist, neg_dist, active_ratio = triplet_loss(\n",
    "                        anchor_emb,\n",
    "                        positive_emb,\n",
    "                        negative_emb,\n",
    "                        margin=margin,\n",
    "                        distance_metric='euclidean',\n",
    "                    )\n",
    "                    \n",
    "                    # Add regularization (was missing in mixed precision path!)\n",
    "                    if use_regularization:\n",
    "                        all_embeddings = torch.cat([anchor_emb, positive_emb, negative_emb], dim=0)\n",
    "                        reg_loss = embedding_regularization_loss(all_embeddings, lambda_reg=0.01)\n",
    "                        loss = loss + reg_loss\n",
    "                    \n",
    "                    loss = loss / accumulation_steps\n",
    "                \n",
    "                # Backward pass\n",
    "                try:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    backward_successful = True\n",
    "                except RuntimeError as e:\n",
    "                    if \"does not require grad\" in str(e):\n",
    "                        gradient_errors += 1\n",
    "                        optimizer.zero_grad()\n",
    "                        backward_successful = False\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise e\n",
    "                \n",
    "                # Optimizer step with accumulation\n",
    "                if backward_successful and (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    try:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # More aggressive clipping\n",
    "                        \n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        # Scheduler step AFTER optimizer step (for OneCycleLR)\n",
    "                        scheduler.step()\n",
    "                        \n",
    "                    except RuntimeError as e:\n",
    "                        if \"No inf checks were recorded\" in str(e):\n",
    "                            gradient_errors += 1\n",
    "                            optimizer.zero_grad()\n",
    "                            scaler = torch.amp.GradScaler(enabled=use_mixed_precision)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise e\n",
    "                        \n",
    "            else:\n",
    "                # Non-mixed precision path\n",
    "                anchor_emb = model(anchor)\n",
    "                positive_emb = model(positive)\n",
    "                negative_emb = model(negative)\n",
    "                \n",
    "                loss, pos_dist, neg_dist, active_ratio = triplet_loss(\n",
    "                    anchor_emb, positive_emb, negative_emb, margin=margin, distance_metric='euclidean',\n",
    "                )\n",
    "                \n",
    "                if use_regularization:\n",
    "                    all_embeddings = torch.cat([anchor_emb, positive_emb, negative_emb], dim=0)\n",
    "                    reg_loss = embedding_regularization_loss(all_embeddings, lambda_reg=0.01)\n",
    "                    loss = loss + reg_loss\n",
    "                \n",
    "                loss = loss / accumulation_steps\n",
    "                \n",
    "                # Backward pass\n",
    "                try:\n",
    "                    loss.backward()\n",
    "                    backward_successful = True\n",
    "                except RuntimeError as e:\n",
    "                    if \"does not require grad\" in str(e):\n",
    "                        gradient_errors += 1\n",
    "                        optimizer.zero_grad()\n",
    "                        backward_successful = False\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise e\n",
    "                \n",
    "                # Optimizer step with accumulation\n",
    "                if backward_successful and (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Scheduler step AFTER optimizer step\n",
    "                    scheduler.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            total_pos_dist += pos_dist.item()\n",
    "            total_neg_dist += neg_dist.item()\n",
    "            total_active_ratio += active_ratio\n",
    "            num_batches += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error in batch {batch_idx}: {str(e)}\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        # Update progress bar\n",
    "        if num_batches > 0:\n",
    "            current_loss = total_loss / num_batches\n",
    "            current_pos_dist = total_pos_dist / num_batches\n",
    "            current_neg_dist = total_neg_dist / num_batches\n",
    "            current_active_ratio = total_active_ratio / num_batches\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']  # Show current LR\n",
    "            \n",
    "            postfix = {\n",
    "                'Loss': f'{current_loss:.3f}',\n",
    "                'Pos': f'{current_pos_dist:.3f}',\n",
    "                'Neg': f'{current_neg_dist:.3f}',\n",
    "                'Active': f'{current_active_ratio:.2%}',\n",
    "                'LR': f'{current_lr:.2e}'\n",
    "            }\n",
    "            \n",
    "            if gradient_errors > 0:\n",
    "                postfix['Errs'] = f'{gradient_errors}'\n",
    "                \n",
    "            pbar.set_postfix(postfix)\n",
    "    \n",
    "    # Handle remaining gradients\n",
    "    if (batch_idx + 1) % accumulation_steps != 0:\n",
    "        if scaler is not None:\n",
    "            try:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()  # Don't forget scheduler here too!\n",
    "            except RuntimeError:\n",
    "                pass\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Don't forget scheduler here too!\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    if gradient_errors > 0:\n",
    "        print(f\"Epoch {epoch}: Encountered {gradient_errors} gradient errors out of {len(dataloader)} batches\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_pos_dist = total_pos_dist / num_batches if num_batches > 0 else 0\n",
    "    avg_neg_dist = total_neg_dist / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    return avg_loss, avg_pos_dist, avg_neg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, auc\n",
    "\n",
    "def evaluate_triplet_val(model, val_loader, device, margin=1.0):\n",
    "    \"\"\"Improved validation evaluation, including additional embedding metrics, AUC, and F1.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    pos_distances = []\n",
    "    neg_distances = []\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            anchor = batch['anchor'].to(device, non_blocking=True)\n",
    "            positive = batch['positive'].to(device, non_blocking=True)\n",
    "            negative = batch['negative'].to(device, non_blocking=True)\n",
    "\n",
    "            anchor_emb = model(anchor)\n",
    "            positive_emb = model(positive)\n",
    "            negative_emb = model(negative)\n",
    "\n",
    "            loss, _, _, _ = triplet_loss( # We only need the loss here, distances are re-calculated with cosine\n",
    "                anchor_emb, positive_emb, negative_emb, margin=margin, distance_metric='euclidean' # 'euclidean' as per original\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Compute distances using 1 - F.cosine_similarity as in compute_embedding_metrics\n",
    "            pos_dist = 1 - F.cosine_similarity(anchor_emb, positive_emb)\n",
    "            neg_dist = 1 - F.cosine_similarity(anchor_emb, negative_emb)\n",
    "\n",
    "            pos_distances.extend(pos_dist.cpu().numpy())\n",
    "            neg_distances.extend(neg_dist.cpu().numpy())\n",
    "\n",
    "            # Store embeddings for additional metrics\n",
    "            embeddings.extend(anchor_emb.cpu().numpy())\n",
    "            embeddings.extend(positive_emb.cpu().numpy())\n",
    "            embeddings.extend(negative_emb.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    pos_distances = np.array(pos_distances)\n",
    "    neg_distances = np.array(neg_distances)\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # Prepare labels and scores for AUC/F1 calculation\n",
    "    # For triplet loss: positive pairs should have lower distances (better matches)\n",
    "    # negative pairs should have higher distances (worse matches)\n",
    "    labels = np.concatenate([np.ones(len(pos_distances)), np.zeros(len(neg_distances))])  # 1 for positive pairs, 0 for negative pairs\n",
    "    distances = np.concatenate([pos_distances, neg_distances])\n",
    "    \n",
    "    # Convert distances to similarity scores (lower distance = higher similarity)\n",
    "    similarities = 1 - distances\n",
    "    \n",
    "    # Calculate AUC-ROC\n",
    "    auc_roc = roc_auc_score(labels, similarities)\n",
    "    \n",
    "    # Calculate AUC-PR\n",
    "    precision, recall, _ = precision_recall_curve(labels, similarities)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    \n",
    "    # Calculate F1 score using optimal threshold\n",
    "    # Find threshold that maximizes F1 score\n",
    "    thresholds = np.linspace(similarities.min(), similarities.max(), 100)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (similarities >= threshold).astype(int)\n",
    "        if len(np.unique(y_pred)) > 1:  # Avoid division by zero\n",
    "            f1_scores.append(f1_score(labels, y_pred))\n",
    "        else:\n",
    "            f1_scores.append(0)\n",
    "    \n",
    "    best_f1 = max(f1_scores)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "    metrics = {\n",
    "        'avg_loss': avg_loss,\n",
    "        \n",
    "        # Classification metrics\n",
    "        'auc_roc': auc_roc,\n",
    "        'auc_pr': auc_pr,\n",
    "        'best_f1': best_f1,\n",
    "        'best_threshold': best_threshold,\n",
    "        \n",
    "        # Distance statistics\n",
    "        'pos_dist_mean': np.mean(pos_distances),\n",
    "        'pos_dist_std': np.std(pos_distances),\n",
    "        'neg_dist_mean': np.mean(neg_distances),\n",
    "        'neg_dist_std': np.std(neg_distances),\n",
    "        'dist_gap': np.mean(neg_distances) - np.mean(pos_distances),\n",
    "\n",
    "        # Margin violations\n",
    "        'margin_violations': np.mean(neg_distances < pos_distances + 0.1), # Assuming 0.1 as a generic margin for this metric\n",
    "\n",
    "        # Distribution overlap\n",
    "        'pos_95_percentile': np.percentile(pos_distances, 95),\n",
    "        'neg_5_percentile': np.percentile(neg_distances, 5),\n",
    "        'distribution_overlap': max(0, np.percentile(pos_distances, 95) - np.percentile(neg_distances, 5)),\n",
    "\n",
    "        # Embedding space quality\n",
    "        'embedding_norm_std': np.std(np.linalg.norm(embeddings, axis=1)),\n",
    "\n",
    "        # Relative improvement\n",
    "        'relative_gap': (np.mean(neg_distances) - np.mean(pos_distances)) / np.mean(pos_distances),\n",
    "\n",
    "        # Separability score (higher is better)\n",
    "        'separability': np.mean(neg_distances > pos_distances)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553edf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history tracking\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_pos_distances': [], 'train_neg_distances': [],\n",
    "    'val_pos_distances': [], 'val_neg_distances': [],\n",
    "    'val_pos_neg_diffs': [],\n",
    "    'learning_rates': [], 'separability': [],\n",
    "    'auc_roc': [], 'best_f1': [],\n",
    "}\n",
    "\n",
    "def train_triplet_model(model, train_loader, val_loader=None, num_epochs=50,\n",
    "                        device='cuda', lr=1e-3, margin=1.0,\n",
    "                        use_mixed_precision=True, accumulation_steps=1):\n",
    "    \"\"\"\n",
    "    Improved training function with better optimization and scheduling.\n",
    "    \"\"\"\n",
    "    global history\n",
    "\n",
    "    # Better optimizer configuration\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=5e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,  # Lower max LR (was 5e-4)\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,  # Longer warmup (was 0.1)\n",
    "        div_factor=50,  # Start even lower (was 25)\n",
    "        final_div_factor=1000\n",
    "    )\n",
    "\n",
    "    scaler = torch.amp.GradScaler(enabled=use_mixed_precision)\n",
    "\n",
    "    initial_margin = margin\n",
    "\n",
    "    best_pos_neg_diff = float('-inf')\n",
    "    best_separability = float('-inf')\n",
    "    best_val_loss = float('inf')\n",
    "    best_auc_roc = float('-inf')\n",
    "    best_f1 = float('-inf')\n",
    "\n",
    "    # --- Print the header once before the training loop ---\n",
    "    header_str = (\n",
    "        \"{:<12} {:<8} {:<8} {:<8} {:<10} {:<10} {:<20} {:<20} {:<10} {:<12} {:<8} {:<8}\"\n",
    "    ).format(\n",
    "        \"Epoch\", \"Loss\", \"Pos\", \"Neg\", \"LR\",\n",
    "        \"Val Loss\", \"Val Pos (Mean/Std)\", \"Val Neg (Mean/Std)\", \"Dist Gap\", \"Separability\", \"AUC\", \"F1\"\n",
    "    )\n",
    "    print(\"\\n\" + header_str)\n",
    "    print(\"-\" * len(header_str))\n",
    "\n",
    "    val_metrics = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        do_log = False\n",
    "        current_margin = initial_margin\n",
    "\n",
    "        # Training phase\n",
    "        train_loss, train_pos_dist, train_neg_dist = train_triplet(\n",
    "            model, train_loader, optimizer, scheduler, device, epoch,\n",
    "            margin=current_margin, scaler=scaler, accumulation_steps=accumulation_steps,\n",
    "            use_regularization=True\n",
    "        )\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Record metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_pos_distances'].append(train_pos_dist)\n",
    "        history['train_neg_distances'].append(train_neg_dist)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "\n",
    "        # Initialize log_msg for the current epoch\n",
    "        epoch_log_values = {\n",
    "            \"epoch\": f\"{epoch+1}/{num_epochs}\",\n",
    "            \"train_loss\": f\"{train_loss:.3f}\",\n",
    "            \"train_pos_dist\": f\"{train_pos_dist:.3f}\",\n",
    "            \"train_neg_dist\": f\"{train_neg_dist:.3f}\",\n",
    "            \"current_lr\": f\"{current_lr:.2e}\"\n",
    "        }\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader is not None:\n",
    "            val_metrics = evaluate_triplet_val(\n",
    "                model, val_loader, device, margin=current_margin\n",
    "            )\n",
    "\n",
    "            # Update history with all new metrics\n",
    "            history['val_loss'].append(val_metrics['avg_loss'])\n",
    "            history['val_pos_distances'].append(val_metrics['pos_dist_mean'])\n",
    "            history['val_neg_distances'].append(val_metrics['neg_dist_mean'])\n",
    "            history['val_pos_neg_diffs'].append(val_metrics['dist_gap'])\n",
    "            history['separability'].append(val_metrics['separability'])\n",
    "            history['auc_roc'].append(val_metrics['auc_roc'])\n",
    "            history['best_f1'].append(val_metrics['best_f1'])\n",
    "\n",
    "            # Append validation metrics to the current epoch's log values\n",
    "            epoch_log_values.update({\n",
    "                \"val_loss\": f\"{val_metrics['avg_loss']:.3f}\",\n",
    "                \"val_pos_mean_std\": f\"{val_metrics['pos_dist_mean']:.3f}/{val_metrics['pos_dist_std']:.3f}\",\n",
    "                \"val_neg_mean_std\": f\"{val_metrics['neg_dist_mean']:.3f}/{val_metrics['neg_dist_std']:.3f}\",\n",
    "                \"dist_gap\": f\"{val_metrics['dist_gap']:.3f}\",\n",
    "                \"separability\": f\"{val_metrics['separability']:.3f}\",\n",
    "                \"auc_roc\": f\"{val_metrics['auc_roc']:.3f}\",\n",
    "                \"best_f1\": f\"{val_metrics['best_f1']:.3f}\",\n",
    "            })\n",
    "\n",
    "            # Early stopping and model saving\n",
    "            star_indicator = \"\"\n",
    "            if val_metrics['dist_gap'] > best_pos_neg_diff:\n",
    "                best_pos_neg_diff = val_metrics['dist_gap']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_metrics': val_metrics, # Save all validation metrics\n",
    "                    'history': history\n",
    "                }, \"best_triplet_model_dist_gap.pth\")\n",
    "                star_indicator += \" *\" # Indicate that a new best model was saved\n",
    "                do_log = True\n",
    "            \n",
    "            if val_metrics['separability'] > best_separability:\n",
    "                best_separability = val_metrics['separability']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_metrics': val_metrics, # Save all validation metrics\n",
    "                    'history': history\n",
    "                }, \"best_triplet_model_separability.pth\")\n",
    "                star_indicator += \" +\" # Indicate that a new best model was saved\n",
    "                do_log = True\n",
    "\n",
    "            if val_metrics['avg_loss'] < best_val_loss:\n",
    "                best_val_loss = val_metrics['avg_loss']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_metrics': val_metrics, # Save all validation metrics\n",
    "                    'history': history\n",
    "                }, \"best_triplet_model_val_loss.pth\")\n",
    "                star_indicator += \" ^\" # Indicate that a new best model was saved\n",
    "                do_log = True\n",
    "\n",
    "            if val_metrics['auc_roc'] > best_auc_roc:\n",
    "                best_auc_roc = val_metrics['auc_roc']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'history': history\n",
    "                }, \"best_triplet_model_auc.pth\")\n",
    "                star_indicator += \" ◆\" # AUC best model\n",
    "                do_log = True\n",
    "\n",
    "            if val_metrics['best_f1'] > best_f1:\n",
    "                best_f1 = val_metrics['best_f1']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'history': history\n",
    "                }, \"best_triplet_model_f1.pth\")\n",
    "                star_indicator += \" ◇\" # F1 best model\n",
    "                do_log = True\n",
    "\n",
    "            # Print formatted epoch log\n",
    "            if do_log:\n",
    "                print(\n",
    "                    (\n",
    "                        \"{:<12} {:<8} {:<8} {:<8} {:<10} {:<10} {:<20} {:<20} {:<10} {:<12} {:<8} {:<8}\"\n",
    "                    ).format(\n",
    "                        epoch_log_values[\"epoch\"],\n",
    "                        epoch_log_values[\"train_loss\"],\n",
    "                        epoch_log_values[\"train_pos_dist\"],\n",
    "                        epoch_log_values[\"train_neg_dist\"],\n",
    "                        epoch_log_values[\"current_lr\"],\n",
    "                        epoch_log_values[\"val_loss\"],\n",
    "                        epoch_log_values[\"val_pos_mean_std\"],\n",
    "                        epoch_log_values[\"val_neg_mean_std\"],\n",
    "                        epoch_log_values[\"dist_gap\"],\n",
    "                        epoch_log_values[\"separability\"],\n",
    "                        epoch_log_values[\"auc_roc\"],\n",
    "                        epoch_log_values[\"best_f1\"]\n",
    "                    ) + star_indicator\n",
    "                )\n",
    "\n",
    "    torch.save({\n",
    "                'epoch': num_epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_metrics': val_metrics, # Save all validation metrics\n",
    "                'history': history\n",
    "            }, \"best_triplet_model_last_epoch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f72ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplet_model(\n",
    "    model, train_loader, val_loader=val_loader, \n",
    "    num_epochs=NUM_EPOCHS, device=device, margin=MARGIN, accumulation_steps=ACCUMULATION_STEPS, lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f00c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = generate_model(model_depth=10, embedding_dim=128, use_attention=True)\n",
    "checkpoint = torch.load(\"best_triplet_model_last_epoch.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_dist_gap = generate_model(model_depth=10, embedding_dim=128, use_attention=True)  \n",
    "checkpoint = torch.load(\"best_triplet_model_dist_gap.pth\")\n",
    "model_dist_gap.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_dist_gap = model_dist_gap.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_separability = generate_model(model_depth=10, embedding_dim=128, use_attention=True)\n",
    "checkpoint = torch.load(\"best_triplet_model_separability.pth\")\n",
    "model_separability.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_separability = model_separability.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_val_loss = generate_model(model_depth=10, embedding_dim=128, use_attention=True)\n",
    "checkpoint = torch.load(\"best_triplet_model_val_loss.pth\")\n",
    "model_val_loss.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_val_loss = model_val_loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_auc = generate_model(model_depth=10, embedding_dim=128, use_attention=True)\n",
    "checkpoint = torch.load(\"best_triplet_model_auc.pth\")\n",
    "model_auc.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_auc = model_auc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12aec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_f1 = generate_model(model_depth=10, embedding_dim=128, use_attention=True)\n",
    "checkpoint = torch.load(\"best_triplet_model_f1.pth\")\n",
    "model_f1.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_f1 = model_f1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history to pickle\n",
    "import pickle\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load history from pickle\n",
    "with open('history.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1873ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_history_interpolated(history, n_epochs=1):\n",
    "    \"\"\"\n",
    "    Plot training history with interpolation for clarity.\n",
    "    Marks 5 best models based on different metrics including AUC and F1.\n",
    "    \n",
    "    Args:\n",
    "        history: Dictionary containing training history with keys:\n",
    "                 'train_loss', 'val_loss', 'pos_distances', 'neg_distances',\n",
    "                 'val_pos_neg_diffs', 'separability', 'auc_roc', 'best_f1'\n",
    "        n_epochs: Number of epochs to average over for interpolation (default=1, no interpolation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def interpolate_data(data, n):\n",
    "        \"\"\"Take the mean at every n epochs.\"\"\"\n",
    "        if n <= 1:\n",
    "            return data, range(1, len(data) + 1)\n",
    "        \n",
    "        interpolated = []\n",
    "        new_x = []\n",
    "        \n",
    "        for i in range(0, len(data) + 1, n):\n",
    "            end_idx = min(i + n, len(data))\n",
    "            chunk = data[i:end_idx]\n",
    "            interpolated.append(np.mean(chunk))\n",
    "            new_x.append(i + len(chunk) // 2 + 1)  # Center of the chunk\n",
    "        \n",
    "        return interpolated, new_x\n",
    "    \n",
    "    # Interpolate all data\n",
    "    train_loss_interp, x_train = interpolate_data(history['train_loss'], n_epochs)\n",
    "    val_loss_interp, x_val = interpolate_data(history['val_loss'], n_epochs)\n",
    "    pos_distances_interp, x_pos = interpolate_data(history['val_pos_distances'], n_epochs)\n",
    "    neg_distances_interp, x_neg = interpolate_data(history['val_neg_distances'], n_epochs)\n",
    "    pos_neg_diffs_interp, x_diff = interpolate_data(history['val_pos_neg_diffs'], n_epochs)\n",
    "    separability_interp, x_sep = interpolate_data(history['separability'], n_epochs)\n",
    "    auc_roc_interp, x_auc = interpolate_data(history['auc_roc'], n_epochs)\n",
    "    best_f1_interp, x_f1 = interpolate_data(history['best_f1'], n_epochs)\n",
    "    \n",
    "    # Use the original x for consistent plotting\n",
    "    x = x_train\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    marker_size = 3\n",
    "    \n",
    "    plt.plot(x, train_loss_interp, label='Training Loss', markersize=marker_size, zorder=0)\n",
    "    plt.plot(x, val_loss_interp, label='Validation Loss', markersize=marker_size, zorder=0)\n",
    "    plt.plot(x, pos_distances_interp, label='Twin Distances', marker='o', markersize=marker_size, linestyle='')\n",
    "    plt.plot(x, neg_distances_interp, label='Non-Twin Distances', marker='o', markersize=marker_size, linestyle='')\n",
    "    plt.plot(x, pos_neg_diffs_interp, label='Pos-Neg Diff', marker='s', markersize=marker_size, linestyle='')\n",
    "    plt.plot(x, separability_interp, label='Separability', marker='^', markersize=marker_size, linestyle='')\n",
    "    plt.plot(x, auc_roc_interp, label='AUC-ROC', marker='d', markersize=marker_size, linestyle='')\n",
    "    plt.plot(x, best_f1_interp, label='F1 Score', marker='*', markersize=marker_size, linestyle='')\n",
    "    \n",
    "    # Find best epochs for each metric\n",
    "    best_val_loss_original = np.argmin(history['val_loss']) + 1\n",
    "    best_pos_neg_diff_original = np.argmax(history['val_pos_neg_diffs']) + 1  # Higher is better\n",
    "    best_separability_original = np.argmax(history['separability']) + 1  # Higher is better\n",
    "    best_auc_original = np.argmax(history['auc_roc']) + 1  # Higher is better\n",
    "    best_f1_original = np.argmax(history['best_f1']) + 1  # Higher is better\n",
    "    \n",
    "    # Get the actual metric values at best epochs\n",
    "    best_val_loss_value = history['val_loss'][best_val_loss_original - 1]\n",
    "    best_pos_neg_diff_value = history['val_pos_neg_diffs'][best_pos_neg_diff_original - 1]\n",
    "    best_separability_value = history['separability'][best_separability_original - 1]\n",
    "    best_auc_value = history['auc_roc'][best_auc_original - 1]\n",
    "    best_f1_value = history['best_f1'][best_f1_original - 1]\n",
    "    \n",
    "    # Find closest interpolated points\n",
    "    best_val_loss_idx = np.argmin(np.abs(np.array(x) - best_val_loss_original))\n",
    "    best_pos_neg_diff_idx = np.argmin(np.abs(np.array(x) - best_pos_neg_diff_original))\n",
    "    best_separability_idx = np.argmin(np.abs(np.array(x) - best_separability_original))\n",
    "    best_auc_idx = np.argmin(np.abs(np.array(x) - best_auc_original))\n",
    "    best_f1_idx = np.argmin(np.abs(np.array(x) - best_f1_original))\n",
    "    \n",
    "    best_epochs = [\n",
    "        (x[best_val_loss_idx], best_val_loss_idx, 'Val Loss', 'red', best_val_loss_value),\n",
    "        (x[best_pos_neg_diff_idx], best_pos_neg_diff_idx, 'Pos-Neg Diff', 'orange', best_pos_neg_diff_value),\n",
    "        (x[best_separability_idx], best_separability_idx, 'Separability', 'purple', best_separability_value),\n",
    "        (x[best_auc_idx], best_auc_idx, 'AUC-ROC', 'blue', best_auc_value),\n",
    "        (x[best_f1_idx], best_f1_idx, 'F1 Score', 'green', best_f1_value)\n",
    "    ]\n",
    "    \n",
    "    # Plot best model lines and labels\n",
    "    for epoch, idx, metric_name, color, metric_value in best_epochs:\n",
    "        plt.plot([epoch, epoch], \n",
    "                 [pos_distances_interp[idx], neg_distances_interp[idx]], \n",
    "                 color=color, linestyle='-', lw=1.5, zorder=2, \n",
    "                 label=f'Best {metric_name} [{metric_value:.4f}] (Epoch {epoch})')\n",
    "        \n",
    "        # Add text annotations\n",
    "        text_offset = 0.02\n",
    "        plt.text(epoch, pos_distances_interp[idx] - text_offset, \n",
    "                 f'{pos_distances_interp[idx]:.4f}', \n",
    "                 fontsize=8, ha='center', va='top', zorder=99, \n",
    "                 bbox=dict(boxstyle=\"round,pad=0.1\", facecolor=color, alpha=0.3))\n",
    "        plt.text(epoch, neg_distances_interp[idx] + text_offset, \n",
    "                 f'{neg_distances_interp[idx]:.4f}', \n",
    "                 fontsize=8, ha='center', va='bottom', zorder=99,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.1\", facecolor=color, alpha=0.3))\n",
    "    \n",
    "    # Draw gray vertical lines for other epochs\n",
    "    best_epoch_set = {epoch for epoch, _, _, _, _ in best_epochs}\n",
    "    for i, epoch in enumerate(x):\n",
    "        if epoch not in best_epoch_set:\n",
    "            plt.plot([epoch, epoch], \n",
    "                     [pos_distances_interp[i], neg_distances_interp[i]], \n",
    "                     color='gray', linestyle='--', lw=0.5, zorder=1)\n",
    "    \n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.xlabel('Epoch', fontsize=10, weight='bold')\n",
    "    plt.ylabel('Loss', fontsize=10, weight='bold')\n",
    "    \n",
    "    # Set x-ticks starting from 1, then every 100 epochs\n",
    "    max_epoch = max(x) if x else 100\n",
    "    xtick_positions = [1] + list(range(100, max_epoch + 1, 100))\n",
    "    plt.xticks(xtick_positions)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_history_interpolated(history, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdf63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, precision_recall_curve, fbeta_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_pairs_from_triplet_dataset(triplet_dataset):\n",
    "    \"\"\"\n",
    "    Create a traditional pairs dataset from triplet dataset for evaluation.\n",
    "    Returns both positive pairs (twins) and negative pairs (non-twins).\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Add positive pairs from twin pairs\n",
    "    for subject1_id, subject2_id in triplet_dataset.twin_pairs:\n",
    "        pairs.append((subject1_id, subject2_id))\n",
    "        labels.append(1)  # Twin pair\n",
    "    \n",
    "    # Add negative pairs by sampling non-twins\n",
    "    # Sample same number of negative pairs as positive pairs for balance\n",
    "    num_negative_pairs = len(triplet_dataset.twin_pairs) * 1\n",
    "    negative_pairs_added = 0\n",
    "    \n",
    "    # Get all possible non-twin combinations\n",
    "    import random\n",
    "    random.seed(42)  # For reproducible evaluation\n",
    "    \n",
    "    while negative_pairs_added < num_negative_pairs:\n",
    "        # Randomly sample two subjects\n",
    "        subject1 = random.choice(triplet_dataset.all_subjects)\n",
    "        subject2 = random.choice(triplet_dataset.all_subjects)\n",
    "        \n",
    "        if subject1 == subject2:\n",
    "            continue\n",
    "            \n",
    "        # Check if they're twins (skip if they are)\n",
    "        is_twin_pair = False\n",
    "        twin_idx = triplet_dataset.subject_to_twin_idx.get(subject1)\n",
    "        if twin_idx is not None:\n",
    "            twin_subject1, twin_subject2 = triplet_dataset.twin_pairs[twin_idx]\n",
    "            if subject2 in [twin_subject1, twin_subject2]:\n",
    "                is_twin_pair = True\n",
    "                \n",
    "        if not is_twin_pair:\n",
    "            # Sort to avoid duplicates\n",
    "            pair = tuple(sorted([subject1, subject2]))\n",
    "            if pair not in [(tuple(sorted([s1, s2])) for s1, s2 in pairs)]:\n",
    "                pairs.append(pair)\n",
    "                labels.append(0)  # Non-twin pair\n",
    "                negative_pairs_added += 1\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "\n",
    "def evaluate(model, test_dataset, device, threshold=None, min_recall=None, \n",
    "             min_precision=None, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate a triplet-trained model using pairwise comparisons with comprehensive threshold analysis.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        test_dataset: TripletBrainDataset for test data\n",
    "        device: Device to run evaluation on\n",
    "        threshold: Fixed threshold to use (if None, will be optimized)\n",
    "        min_recall: Minimum required recall (0-1)\n",
    "        min_precision: Minimum required precision (0-1)\n",
    "        batch_size: Batch size for evaluation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create pairs from triplet dataset\n",
    "    print(\"Creating evaluation pairs from triplet dataset...\")\n",
    "    pairs, labels = create_pairs_from_triplet_dataset(test_dataset)\n",
    "    \n",
    "    print(f\"Created {len(pairs)} pairs for evaluation:\")\n",
    "    print(f\"  Positive pairs (twins): {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "    print(f\"  Negative pairs (non-twins): {len(labels) - sum(labels)} ({(len(labels) - sum(labels))/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    all_distances = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process pairs in batches\n",
    "    print(\"Computing embeddings and distances...\")\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(pairs), batch_size), desc=\"Evaluating\"):\n",
    "            batch_pairs = pairs[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            \n",
    "            batch_images1 = []\n",
    "            batch_images2 = []\n",
    "            \n",
    "            # Load images for this batch\n",
    "            for subject1_id, subject2_id in batch_pairs:\n",
    "                try:\n",
    "                    img1 = test_dataset._load_image(subject1_id)\n",
    "                    img2 = test_dataset._load_image(subject2_id)\n",
    "                    \n",
    "                    # Convert to tensors and add channel dimension\n",
    "                    img1 = torch.tensor(img1, dtype=torch.float32).unsqueeze(0)  # [1, D, H, W]\n",
    "                    img2 = torch.tensor(img2, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                    img1.to(device)\n",
    "                    img2.to(device)\n",
    "                    \n",
    "                    # Apply transforms if available\n",
    "                    if test_dataset.transform:\n",
    "                        img1 = test_dataset.transform(img1)\n",
    "                        img2 = test_dataset.transform(img2)\n",
    "                    \n",
    "                    batch_images1.append(img1)\n",
    "                    batch_images2.append(img2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading images for pair {subject1_id}, {subject2_id}: {e}\")\n",
    "                    # Use dummy tensors as fallback\n",
    "                    dummy_shape = (1, 64, 64, 64)  # Adjust based on your data\n",
    "                    batch_images1.append(torch.zeros(dummy_shape))\n",
    "                    batch_images2.append(torch.zeros(dummy_shape))\n",
    "            \n",
    "            # Stack into batch tensors\n",
    "            batch_img1 = torch.stack(batch_images1).to(device).float()\n",
    "            batch_img2 = torch.stack(batch_images2).to(device).float()\n",
    "            \n",
    "            embeddings1 = model(batch_img1)\n",
    "            embeddings2 = model(batch_img2)\n",
    "            \n",
    "            # Compute distances\n",
    "            # distances = F.pairwise_distance(embeddings1, embeddings2, p=2)\n",
    "            distances = 1 - F.cosine_similarity(embeddings1, embeddings2, dim=1)\n",
    "            \n",
    "            all_distances.extend(distances.cpu().numpy())\n",
    "            all_labels.extend(batch_labels)\n",
    "    \n",
    "    all_distances = np.array(all_distances)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    print(f\"Distance statistics:\")\n",
    "    print(f\"  Min: {all_distances.min():.4f}\")\n",
    "    print(f\"  Max: {all_distances.max():.4f}\")\n",
    "    print(f\"  Mean: {all_distances.mean():.4f}\")\n",
    "    print(f\"  Std: {all_distances.std():.4f}\")\n",
    "    \n",
    "    print(f\"Label distribution:\")\n",
    "    print(f\"  Twins (1): {np.sum(all_labels == 1)} ({np.mean(all_labels == 1)*100:.1f}%)\")\n",
    "    print(f\"  Non-twins (0): {np.sum(all_labels == 0)} ({np.mean(all_labels == 0)*100:.1f}%)\")\n",
    "\n",
    "    # Find optimal threshold if not provided\n",
    "    if threshold is None:\n",
    "        print(\"Finding optimal threshold...\")\n",
    "        \n",
    "        # Method 1: Use reasonable distance range instead of ROC thresholds\n",
    "        min_dist, max_dist = all_distances.min(), all_distances.max()\n",
    "        distance_range = max_dist - min_dist\n",
    "        \n",
    "        # Create candidate thresholds within the actual distance range\n",
    "        n_thresholds = 1000\n",
    "        candidate_thresholds = np.linspace(min_dist + 0.01 * distance_range, \n",
    "                                         max_dist - 0.01 * distance_range, \n",
    "                                         n_thresholds)\n",
    "        \n",
    "        print(f\"Searching thresholds in range [{candidate_thresholds[0]:.4f}, {candidate_thresholds[-1]:.4f}]\")\n",
    "        \n",
    "        # Check for recall or precision constraints\n",
    "        if min_recall is not None:\n",
    "            print(f\"Applying minimum recall constraint: {min_recall:.3f}\")\n",
    "            threshold = find_threshold_for_min_recall(all_distances, all_labels, candidate_thresholds, min_recall)\n",
    "            final_method = f'min_recall_{min_recall:.3f}'\n",
    "            \n",
    "        elif min_precision is not None:\n",
    "            print(f\"Applying minimum precision constraint: {min_precision:.3f}\")\n",
    "            threshold = find_threshold_for_min_precision(all_distances, all_labels, candidate_thresholds, min_precision)\n",
    "            final_method = f'min_precision_{min_precision:.3f}'\n",
    "            \n",
    "        else:\n",
    "            # Original optimization methods when no constraints are specified\n",
    "            methods = {\n",
    "                'f1': lambda y_true, y_pred: f1_score(y_true, y_pred, zero_division=0),\n",
    "                'f0.5': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=0.5, zero_division=0),\n",
    "                'f2': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2.0, zero_division=0),\n",
    "                'balanced_accuracy': lambda y_true, y_pred: 0.5 * (\n",
    "                    recall_score(y_true, y_pred, pos_label=1, zero_division=0) + \n",
    "                    recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "                ),\n",
    "                'youden_j': lambda y_true, y_pred: (\n",
    "                    recall_score(y_true, y_pred, pos_label=1, zero_division=0) + \n",
    "                    recall_score(y_true, y_pred, pos_label=0, zero_division=0) - 1\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            best_thresholds = {}\n",
    "            best_scores = {}\n",
    "            \n",
    "            for method_name, score_func in methods.items():\n",
    "                scores = []\n",
    "                for thresh in candidate_thresholds:\n",
    "                    predictions = (all_distances < thresh).astype(int)\n",
    "                    # Skip if all predictions are the same class\n",
    "                    if len(np.unique(predictions)) <= 1:\n",
    "                        scores.append(0)\n",
    "                    else:\n",
    "                        score = score_func(all_labels, predictions)\n",
    "                        scores.append(score)\n",
    "                \n",
    "                best_idx = np.argmax(scores)\n",
    "                best_thresholds[method_name] = candidate_thresholds[best_idx]\n",
    "                best_scores[method_name] = scores[best_idx]\n",
    "                \n",
    "                print(f\"{method_name}: threshold={best_thresholds[method_name]:.4f}, score={best_scores[method_name]:.4f}\")\n",
    "            \n",
    "            # Method 2: Statistical approach - find threshold that best separates the classes\n",
    "            if len(np.unique(all_labels)) == 2:\n",
    "                twin_distances = all_distances[all_labels == 1]\n",
    "                non_twin_distances = all_distances[all_labels == 0]\n",
    "                \n",
    "                # Option A: Midpoint between means\n",
    "                midpoint_threshold = (twin_distances.mean() + non_twin_distances.mean()) / 2\n",
    "                \n",
    "                # Option B: Intersection of gaussian fits (if distributions overlap)\n",
    "                try:\n",
    "                    from scipy import stats\n",
    "                    # Fit normal distributions\n",
    "                    twin_params = stats.norm.fit(twin_distances)\n",
    "                    non_twin_params = stats.norm.fit(non_twin_distances)\n",
    "                    \n",
    "                    # Find intersection point (approximate)\n",
    "                    search_range = np.linspace(min_dist, max_dist, 1000)\n",
    "                    twin_pdf = stats.norm.pdf(search_range, *twin_params)\n",
    "                    non_twin_pdf = stats.norm.pdf(search_range, *non_twin_params)\n",
    "                    \n",
    "                    # Find where PDFs are closest\n",
    "                    diff = np.abs(twin_pdf - non_twin_pdf)\n",
    "                    intersection_threshold = search_range[np.argmin(diff)]\n",
    "                    \n",
    "                    print(f\"Statistical thresholds:\")\n",
    "                    print(f\"  Midpoint: {midpoint_threshold:.4f}\")\n",
    "                    print(f\"  Intersection: {intersection_threshold:.4f}\")\n",
    "                    \n",
    "                    # Add to candidates\n",
    "                    best_thresholds['midpoint'] = midpoint_threshold\n",
    "                    best_thresholds['intersection'] = intersection_threshold\n",
    "                    \n",
    "                except ImportError:\n",
    "                    print(f\"Statistical threshold (midpoint): {midpoint_threshold:.4f}\")\n",
    "                    best_thresholds['midpoint'] = midpoint_threshold\n",
    "            \n",
    "            # Select the best method based on F1 score (or choose your preferred metric)\n",
    "            preferred_methods = ['f1', 'f0.5', 'youden_j', 'midpoint', 'balanced_accuracy']\n",
    "            \n",
    "            final_threshold = None\n",
    "            final_method = None\n",
    "            final_score = -1\n",
    "            \n",
    "            for method in preferred_methods:\n",
    "                if method in best_thresholds:\n",
    "                    thresh = best_thresholds[method]\n",
    "                    predictions = (all_distances < thresh).astype(int)\n",
    "                    \n",
    "                    if len(np.unique(predictions)) > 1:\n",
    "                        f1_test = f1_score(all_labels, predictions, zero_division=0)\n",
    "                        if f1_test > final_score:\n",
    "                            final_score = f1_test\n",
    "                            final_threshold = thresh\n",
    "                            final_method = method\n",
    "            \n",
    "            if final_threshold is None:\n",
    "                # Fallback: use midpoint\n",
    "                final_threshold = (all_distances.min() + all_distances.max()) / 2\n",
    "                final_method = 'fallback_midpoint'\n",
    "            \n",
    "            threshold = final_threshold\n",
    "        \n",
    "        print(f\"\\nSelected method: {final_method}\")\n",
    "        print(f\"Final threshold: {threshold:.4f}\")\n",
    "        \n",
    "        # Plot comprehensive threshold analysis\n",
    "        plot_threshold_analysis(all_distances, all_labels, threshold, candidate_thresholds, \n",
    "                              final_method, min_recall, min_precision)\n",
    "\n",
    "    # Make final predictions\n",
    "    all_predictions = (all_distances < threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(all_predictions == all_labels)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(all_labels, -all_distances)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"Threshold: {threshold:.4f}\")\n",
    "    \n",
    "    # Check if constraints were met\n",
    "    if min_recall is not None:\n",
    "        constraint_met = recall >= min_recall\n",
    "        print(f\"Minimum recall constraint ({min_recall:.3f}): {'✓ MET' if constraint_met else '✗ NOT MET'}\")\n",
    "    \n",
    "    if min_precision is not None:\n",
    "        constraint_met = precision >= min_precision\n",
    "        print(f\"Minimum precision constraint ({min_precision:.3f}): {'✓ MET' if constraint_met else '✗ NOT MET'}\")\n",
    "    \n",
    "    print(f\"\\nPrediction breakdown:\")\n",
    "    print(f\"Predicted twins: {np.sum(all_predictions == 1)} ({np.mean(all_predictions == 1)*100:.1f}%)\")\n",
    "    print(f\"Predicted non-twins: {np.sum(all_predictions == 0)} ({np.mean(all_predictions == 0)*100:.1f}%)\")\n",
    "\n",
    "    # Visualization plots\n",
    "    plot_evaluation_results(all_distances, all_labels, all_predictions, threshold, \n",
    "                           fpr, tpr, auc_score)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc_score,\n",
    "        'threshold': threshold,\n",
    "        'all_distances': all_distances,\n",
    "        'all_labels': all_labels,\n",
    "        'all_predictions': all_predictions,\n",
    "        'num_pairs': len(pairs)\n",
    "    }\n",
    "\n",
    "\n",
    "def find_optimal_threshold_comprehensive(all_distances, all_labels, candidate_thresholds):\n",
    "    \"\"\"Comprehensive threshold selection with multiple methods.\"\"\"\n",
    "    \n",
    "    methods = {\n",
    "        'f1': lambda y_true, y_pred: f1_score(y_true, y_pred, zero_division=0),\n",
    "        'f0.5': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=0.5, zero_division=0),\n",
    "        'f2': lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=2.0, zero_division=0),\n",
    "        'balanced_accuracy': lambda y_true, y_pred: 0.5 * (\n",
    "            recall_score(y_true, y_pred, pos_label=1, zero_division=0) + \n",
    "            recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "        ),\n",
    "        'youden_j': lambda y_true, y_pred: (\n",
    "            recall_score(y_true, y_pred, pos_label=1, zero_division=0) + \n",
    "            recall_score(y_true, y_pred, pos_label=0, zero_division=0) - 1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    best_thresholds = {}\n",
    "    best_scores = {}\n",
    "    \n",
    "    for method_name, score_func in methods.items():\n",
    "        scores = []\n",
    "        for thresh in candidate_thresholds:\n",
    "            predictions = (all_distances < thresh).astype(int)\n",
    "            if len(np.unique(predictions)) <= 1:\n",
    "                scores.append(0)\n",
    "            else:\n",
    "                score = score_func(all_labels, predictions)\n",
    "                scores.append(score)\n",
    "        \n",
    "        best_idx = np.argmax(scores)\n",
    "        best_thresholds[method_name] = candidate_thresholds[best_idx]\n",
    "        best_scores[method_name] = scores[best_idx]\n",
    "    \n",
    "    # Statistical approach\n",
    "    if len(np.unique(all_labels)) == 2:\n",
    "        twin_distances = all_distances[all_labels == 1]\n",
    "        non_twin_distances = all_distances[all_labels == 0]\n",
    "        \n",
    "        midpoint_threshold = (twin_distances.mean() + non_twin_distances.mean()) / 2\n",
    "        best_thresholds['midpoint'] = midpoint_threshold\n",
    "    \n",
    "    # Select the best method based on F1 score\n",
    "    preferred_methods = ['f1', 'f0.5', 'youden_j', 'midpoint', 'balanced_accuracy']\n",
    "    \n",
    "    final_threshold = None\n",
    "    final_score = -1\n",
    "    \n",
    "    for method in preferred_methods:\n",
    "        if method in best_thresholds:\n",
    "            thresh = best_thresholds[method]\n",
    "            predictions = (all_distances < thresh).astype(int)\n",
    "            \n",
    "            if len(np.unique(predictions)) > 1:\n",
    "                f1_test = f1_score(all_labels, predictions, zero_division=0)\n",
    "                if f1_test > final_score:\n",
    "                    final_score = f1_test\n",
    "                    final_threshold = thresh\n",
    "    \n",
    "    if final_threshold is None:\n",
    "        final_threshold = (all_distances.min() + all_distances.max()) / 2\n",
    "    \n",
    "    return final_threshold\n",
    "\n",
    "\n",
    "def find_threshold_for_min_recall(distances, labels, candidate_thresholds, min_recall):\n",
    "    \"\"\"Find the threshold that achieves minimum recall while maximizing precision.\"\"\"\n",
    "    valid_thresholds = []\n",
    "    corresponding_precisions = []\n",
    "    corresponding_f1s = []\n",
    "    \n",
    "    for thresh in candidate_thresholds:\n",
    "        predictions = (distances < thresh).astype(int)\n",
    "        \n",
    "        # Skip if all predictions are the same class\n",
    "        if len(np.unique(predictions)) <= 1:\n",
    "            continue\n",
    "            \n",
    "        recall = recall_score(labels, predictions, zero_division=0)\n",
    "        \n",
    "        if recall >= min_recall:\n",
    "            precision = precision_score(labels, predictions, zero_division=0)\n",
    "            f1 = f1_score(labels, predictions, zero_division=0)\n",
    "            \n",
    "            valid_thresholds.append(thresh)\n",
    "            corresponding_precisions.append(precision)\n",
    "            corresponding_f1s.append(f1)\n",
    "    \n",
    "    if not valid_thresholds:\n",
    "        print(f\"Warning: No threshold found that achieves minimum recall of {min_recall:.3f}\")\n",
    "        print(\"Using threshold that maximizes recall instead\")\n",
    "        \n",
    "        best_recall = -1\n",
    "        best_threshold = None\n",
    "        \n",
    "        for thresh in candidate_thresholds:\n",
    "            predictions = (distances < thresh).astype(int)\n",
    "            if len(np.unique(predictions)) > 1:\n",
    "                recall = recall_score(labels, predictions, zero_division=0)\n",
    "                if recall > best_recall:\n",
    "                    best_recall = recall\n",
    "                    best_threshold = thresh\n",
    "        \n",
    "        return best_threshold if best_threshold is not None else candidate_thresholds[len(candidate_thresholds)//2]\n",
    "    \n",
    "    # Among valid thresholds, choose the one with highest precision (or F1 as tiebreaker)\n",
    "    best_idx = np.argmax(corresponding_precisions)\n",
    "    selected_threshold = valid_thresholds[best_idx]\n",
    "    \n",
    "    print(f\"Found {len(valid_thresholds)} thresholds meeting minimum recall constraint\")\n",
    "    print(f\"Selected threshold: {selected_threshold:.4f} (precision: {corresponding_precisions[best_idx]:.4f}, F1: {corresponding_f1s[best_idx]:.4f})\")\n",
    "    \n",
    "    return selected_threshold\n",
    "\n",
    "\n",
    "def find_threshold_for_min_precision(distances, labels, candidate_thresholds, min_precision):\n",
    "    \"\"\"Find the threshold that achieves minimum precision while maximizing recall.\"\"\"\n",
    "    valid_thresholds = []\n",
    "    corresponding_recalls = []\n",
    "    corresponding_f1s = []\n",
    "    \n",
    "    for thresh in candidate_thresholds:\n",
    "        predictions = (distances < thresh).astype(int)\n",
    "        \n",
    "        # Skip if all predictions are the same class\n",
    "        if len(np.unique(predictions)) <= 1:\n",
    "            continue\n",
    "            \n",
    "        precision = precision_score(labels, predictions, zero_division=0)\n",
    "        \n",
    "        if precision >= min_precision:\n",
    "            recall = recall_score(labels, predictions, zero_division=0)\n",
    "            f1 = f1_score(labels, predictions, zero_division=0)\n",
    "            \n",
    "            valid_thresholds.append(thresh)\n",
    "            corresponding_recalls.append(recall)\n",
    "            corresponding_f1s.append(f1)\n",
    "    \n",
    "    if not valid_thresholds:\n",
    "        print(f\"Warning: No threshold found that achieves minimum precision of {min_precision:.3f}\")\n",
    "        print(\"Using threshold that maximizes precision instead\")\n",
    "        \n",
    "        best_precision = -1\n",
    "        best_threshold = None\n",
    "        \n",
    "        for thresh in candidate_thresholds:\n",
    "            predictions = (distances < thresh).astype(int)\n",
    "            if len(np.unique(predictions)) > 1:\n",
    "                precision = precision_score(labels, predictions, zero_division=0)\n",
    "                if precision > best_precision:\n",
    "                    best_precision = precision\n",
    "                    best_threshold = thresh\n",
    "        \n",
    "        return best_threshold if best_threshold is not None else candidate_thresholds[len(candidate_thresholds)//2]\n",
    "    \n",
    "    # Among valid thresholds, choose the one with highest recall (or F1 as tiebreaker)\n",
    "    best_idx = np.argmax(corresponding_recalls)\n",
    "    selected_threshold = valid_thresholds[best_idx]\n",
    "    \n",
    "    print(f\"Found {len(valid_thresholds)} thresholds meeting minimum precision constraint\")\n",
    "    print(f\"Selected threshold: {selected_threshold:.4f} (recall: {corresponding_recalls[best_idx]:.4f}, F1: {corresponding_f1s[best_idx]:.4f})\")\n",
    "    \n",
    "    return selected_threshold\n",
    "\n",
    "\n",
    "def plot_threshold_analysis(all_distances, all_labels, threshold, candidate_thresholds, \n",
    "                           final_method, min_recall=None, min_precision=None):\n",
    "    \"\"\"Plot comprehensive threshold analysis.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Distance distributions\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(all_distances[all_labels == 0], bins=50, alpha=0.7, label='Non-twins', density=True)\n",
    "    plt.hist(all_distances[all_labels == 1], bins=50, alpha=0.7, label='Twins', density=True)\n",
    "    plt.axvline(threshold, color='red', linestyle='--', label=f'Selected: {threshold:.4f}')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distance Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Threshold optimization curves\n",
    "    plt.subplot(2, 3, 2)\n",
    "    methods = ['precision', 'recall', 'f1']\n",
    "    sample_thresholds = candidate_thresholds[::10]\n",
    "    \n",
    "    for method in methods:\n",
    "        scores = []\n",
    "        for thresh in sample_thresholds:\n",
    "            predictions = (all_distances < thresh).astype(int)\n",
    "            if len(np.unique(predictions)) > 1:\n",
    "                if method == 'precision':\n",
    "                    score = precision_score(all_labels, predictions, zero_division=0)\n",
    "                elif method == 'recall':\n",
    "                    score = recall_score(all_labels, predictions, zero_division=0)\n",
    "                else:  # f1\n",
    "                    score = f1_score(all_labels, predictions, zero_division=0)\n",
    "            else:\n",
    "                score = 0\n",
    "            scores.append(score)\n",
    "        \n",
    "        plt.plot(sample_thresholds, scores, label=method, alpha=0.7)\n",
    "    \n",
    "    plt.axvline(threshold, color='red', linestyle='--', label=f'Selected: {threshold:.4f}')\n",
    "    \n",
    "    # Add constraint lines if applicable\n",
    "    if min_recall is not None:\n",
    "        plt.axhline(min_recall, color='orange', linestyle=':', alpha=0.7, label=f'Min recall: {min_recall:.3f}')\n",
    "    if min_precision is not None:\n",
    "        plt.axhline(min_precision, color='purple', linestyle=':', alpha=0.7, label=f'Min precision: {min_precision:.3f}')\n",
    "    \n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metrics vs Threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Prediction fraction vs threshold\n",
    "    plt.subplot(2, 3, 3)\n",
    "    prediction_fractions = []\n",
    "    for thresh in sample_thresholds:\n",
    "        preds = (all_distances < thresh).astype(int)\n",
    "        prediction_fractions.append(np.mean(preds))\n",
    "    \n",
    "    plt.plot(sample_thresholds, prediction_fractions)\n",
    "    plt.axvline(threshold, color='red', linestyle='--', label=f'Selected: {threshold:.4f}')\n",
    "    plt.axhline(0.5, color='gray', linestyle=':', alpha=0.5, label='50% threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Fraction Predicted as Twins')\n",
    "    plt.title('Prediction Distribution vs Threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ROC curve comparison\n",
    "    plt.subplot(2, 3, 4)\n",
    "    fpr, tpr, _ = roc_curve(all_labels, -all_distances)\n",
    "    plt.plot(fpr, tpr, label=f'ROC (AUC={auc(fpr, tpr):.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    \n",
    "    # Mark the selected threshold point\n",
    "    thresh_predictions = (all_distances < threshold).astype(int)\n",
    "    thresh_fpr = np.sum((thresh_predictions == 1) & (all_labels == 0)) / np.sum(all_labels == 0)\n",
    "    thresh_tpr = np.sum((thresh_predictions == 1) & (all_labels == 1)) / np.sum(all_labels == 1)\n",
    "    plt.plot(thresh_fpr, thresh_tpr, 'ro', markersize=8, label=f'Selected point')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    plt.subplot(2, 3, 5)\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(all_labels, -all_distances)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    \n",
    "    # Mark selected threshold\n",
    "    thresh_precision = precision_score(all_labels, thresh_predictions, zero_division=0)\n",
    "    thresh_recall = recall_score(all_labels, thresh_predictions, zero_division=0)\n",
    "    plt.plot(thresh_recall, thresh_precision, 'ro', markersize=8, label=f'Selected point')\n",
    "    \n",
    "    # Add constraint lines if applicable\n",
    "    if min_recall is not None:\n",
    "        plt.axvline(min_recall, color='orange', linestyle=':', alpha=0.7, label=f'Min recall: {min_recall:.3f}')\n",
    "    if min_precision is not None:\n",
    "        plt.axhline(min_precision, color='purple', linestyle=':', alpha=0.7, label=f'Min precision: {min_precision:.3f}')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Performance metrics for selected threshold\n",
    "    plt.subplot(2, 3, 6)\n",
    "    thresh_predictions = (all_distances < threshold).astype(int)\n",
    "    metrics = {\n",
    "        'Accuracy': np.mean(thresh_predictions == all_labels),\n",
    "        'Precision': precision_score(all_labels, thresh_predictions, zero_division=0),\n",
    "        'Recall': recall_score(all_labels, thresh_predictions, zero_division=0),\n",
    "        'F1': f1_score(all_labels, thresh_predictions, zero_division=0),\n",
    "        'F0.5': fbeta_score(all_labels, thresh_predictions, beta=0.5, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    metric_names = list(metrics.keys())\n",
    "    metric_values = list(metrics.values())\n",
    "    bars = plt.bar(metric_names, metric_values)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f'Performance Metrics\\n({final_method})')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_evaluation_results(all_distances, all_labels, all_predictions, threshold, \n",
    "                           fpr, tpr, auc_score):\n",
    "    \"\"\"Plot the final evaluation results.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # ROC Curve\n",
    "    axes[0, 0].plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.4f})')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'k--')\n",
    "    axes[0, 0].set_xlim([0.0, 1.0])\n",
    "    axes[0, 0].set_ylim([0.0, 1.05])\n",
    "    axes[0, 0].set_xlabel('False Positive Rate')\n",
    "    axes[0, 0].set_ylabel('True Positive Rate')\n",
    "    axes[0, 0].set_title('ROC Curve')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precisions, recalls, pr_thresholds = precision_recall_curve(all_labels, -all_distances)\n",
    "    axes[0, 1].plot(recalls, precisions, marker='.')\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision-Recall Curve')\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=[\"Non-Twin\", \"Twin\"], \n",
    "                yticklabels=[\"Non-Twin\", \"Twin\"],\n",
    "                ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(\"Confusion Matrix\")\n",
    "    axes[1, 0].set_xlabel(\"Predicted\")\n",
    "    axes[1, 0].set_ylabel(\"Actual\")\n",
    "    \n",
    "    # Distance distributions\n",
    "    axes[1, 1].hist(all_distances[all_labels == 0], bins=50, alpha=0.7, label='Non-twins', density=True)\n",
    "    axes[1, 1].hist(all_distances[all_labels == 1], bins=50, alpha=0.7, label='Twins', density=True)\n",
    "    axes[1, 1].axvline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "    axes[1, 1].set_xlabel('Distance')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Distance Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_triplet_datasets(*datasets):\n",
    "    \"\"\"\n",
    "    Combine multiple TripletBrainDataset instances into one, preserving all metadata.\n",
    "    This properly merges twin_pairs, all_subjects, and other required attributes.\n",
    "    \"\"\"\n",
    "    if not datasets:\n",
    "        raise ValueError(\"At least one dataset must be provided\")\n",
    "    \n",
    "    # Use the first dataset as a template\n",
    "    first_dataset = datasets[0]\n",
    "    \n",
    "    # Create a new combined dataset with the same structure\n",
    "    class CombinedTripletDataset:\n",
    "        def __init__(self, datasets):\n",
    "            self.datasets = datasets\n",
    "            self.transform = first_dataset.transform if hasattr(first_dataset, 'transform') else None\n",
    "            \n",
    "            # Combine twin pairs from all datasets\n",
    "            self.twin_pairs = []\n",
    "            self.all_subjects = set()\n",
    "            self.subject_to_twin_idx = {}\n",
    "            \n",
    "            # Combine data from all datasets\n",
    "            for dataset in datasets:\n",
    "                if hasattr(dataset, 'twin_pairs'):\n",
    "                    # Add twin pairs with updated indices\n",
    "                    start_idx = len(self.twin_pairs)\n",
    "                    for i, (subj1, subj2) in enumerate(dataset.twin_pairs):\n",
    "                        self.twin_pairs.append((subj1, subj2))\n",
    "                        # Update subject to twin index mapping\n",
    "                        self.subject_to_twin_idx[subj1] = start_idx + i\n",
    "                        self.subject_to_twin_idx[subj2] = start_idx + i\n",
    "                \n",
    "                if hasattr(dataset, 'all_subjects'):\n",
    "                    if isinstance(dataset.all_subjects, (list, tuple)):\n",
    "                        self.all_subjects.update(dataset.all_subjects)\n",
    "                    elif isinstance(dataset.all_subjects, set):\n",
    "                        self.all_subjects.update(dataset.all_subjects)\n",
    "            \n",
    "            # Convert back to list for compatibility\n",
    "            self.all_subjects = list(self.all_subjects)\n",
    "            \n",
    "            # Combine any other attributes that might be needed\n",
    "            self._combine_other_attributes(datasets)\n",
    "            \n",
    "            print(f\"Combined dataset statistics:\")\n",
    "            print(f\"  Total twin pairs: {len(self.twin_pairs)}\")\n",
    "            print(f\"  Total subjects: {len(self.all_subjects)}\")\n",
    "        \n",
    "        def _combine_other_attributes(self, datasets):\n",
    "            \"\"\"Combine other dataset attributes that might be needed.\"\"\"\n",
    "            # Copy common attributes from the first dataset\n",
    "            for attr_name in ['data_dir', 'image_size', 'device']:\n",
    "                if hasattr(datasets[0], attr_name):\n",
    "                    setattr(self, attr_name, getattr(datasets[0], attr_name))\n",
    "            \n",
    "            # If datasets have triplets, combine them too\n",
    "            if hasattr(datasets[0], 'triplets'):\n",
    "                self.triplets = []\n",
    "                for dataset in datasets:\n",
    "                    if hasattr(dataset, 'triplets'):\n",
    "                        self.triplets.extend(dataset.triplets)\n",
    "        \n",
    "        def _load_image(self, subject_id):\n",
    "            \"\"\"Load image by delegating to the appropriate original dataset.\"\"\"\n",
    "            # Try to find which dataset contains this subject\n",
    "            for dataset in self.datasets:\n",
    "                if hasattr(dataset, '_load_image'):\n",
    "                    try:\n",
    "                        return dataset._load_image(subject_id)\n",
    "                    except (FileNotFoundError, KeyError, Exception):\n",
    "                        continue\n",
    "            \n",
    "            # If not found in any dataset, raise an error\n",
    "            raise FileNotFoundError(f\"Subject {subject_id} not found in any of the combined datasets\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            \"\"\"Return total length across all datasets.\"\"\"\n",
    "            return sum(len(dataset) for dataset in self.datasets)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            \"\"\"Get item by finding the appropriate dataset and adjusting index.\"\"\"\n",
    "            current_idx = idx\n",
    "            for dataset in self.datasets:\n",
    "                if current_idx < len(dataset):\n",
    "                    return dataset[current_idx]\n",
    "                current_idx -= len(dataset)\n",
    "            raise IndexError(\"Index out of range\")\n",
    "    \n",
    "    return CombinedTripletDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e02a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = combine_triplet_datasets(val_dataset, test_dataset)\n",
    "evaluate(model, combined_dataset, device,\n",
    "         threshold=None, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = evaluate(model, test_dataset, device=device)\n",
    "model_dist_gap_metrics = evaluate(model_dist_gap, test_dataset, device=device)\n",
    "model_separability_metrics = evaluate(model_separability, test_dataset, device=device)\n",
    "model_val_loss_metrics = evaluate(model_val_loss, test_dataset, device=device)\n",
    "model_auc_metrics = evaluate(model_auc, test_dataset, device=device)\n",
    "model_f1_metrics = evaluate(model_f1, test_dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_evaluation_results(**kwargs):\n",
    "    \"\"\"\n",
    "    Combine multiple evaluation result dictionaries into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        **kwargs: Named evaluation result dictionaries from the evaluate() function.\n",
    "                 Each kwarg should be in format: experiment_name=evaluation_result_dict\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with experiments as rows and metrics as columns\n",
    "    \n",
    "    Example:\n",
    "        # Assuming you have evaluation results from different experiments\n",
    "        baseline_results = evaluate(model1, test_dataset, device)\n",
    "        improved_results = evaluate(model2, test_dataset, device)\n",
    "        constrained_results = evaluate(model3, test_dataset, device, min_recall=0.8)\n",
    "        \n",
    "        # Combine them into a DataFrame\n",
    "        df = combine_evaluation_results(\n",
    "            baseline=baseline_results,\n",
    "            improved=improved_results,\n",
    "            constrained=constrained_results\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    if not kwargs:\n",
    "        raise ValueError(\"At least one evaluation result dictionary must be provided\")\n",
    "    \n",
    "    # Define the metrics we want to extract (excluding arrays and detailed data)\n",
    "    metrics_to_extract = [\n",
    "        'accuracy', 'precision', 'recall', 'f1', 'auc', 'threshold', 'num_pairs'\n",
    "    ]\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    experiment_names = []\n",
    "    rows_data = []\n",
    "    \n",
    "    for experiment_name, eval_results in kwargs.items():\n",
    "        if not isinstance(eval_results, dict):\n",
    "            raise ValueError(f\"Evaluation result for '{experiment_name}' must be a dictionary\")\n",
    "        \n",
    "        experiment_names.append(experiment_name)\n",
    "        row_data = {}\n",
    "        \n",
    "        # Extract scalar metrics\n",
    "        for metric in metrics_to_extract:\n",
    "            if metric in eval_results:\n",
    "                value = eval_results[metric]\n",
    "                # Handle numpy types\n",
    "                if isinstance(value, (np.integer, np.floating)):\n",
    "                    value = value.item()\n",
    "                row_data[metric] = value\n",
    "            else:\n",
    "                row_data[metric] = None\n",
    "        \n",
    "        # Add some derived metrics if the arrays are available\n",
    "        if 'all_distances' in eval_results and 'all_labels' in eval_results:\n",
    "            distances = eval_results['all_distances']\n",
    "            labels = eval_results['all_labels']\n",
    "            \n",
    "            # Add distance statistics\n",
    "            row_data['mean_distance'] = np.mean(distances)\n",
    "            row_data['std_distance'] = np.std(distances)\n",
    "            row_data['min_distance'] = np.min(distances)\n",
    "            row_data['max_distance'] = np.max(distances)\n",
    "            \n",
    "            # Add class-specific distance statistics\n",
    "            if len(np.unique(labels)) == 2:\n",
    "                twin_distances = distances[labels == 1]\n",
    "                non_twin_distances = distances[labels == 0]\n",
    "                \n",
    "                if len(twin_distances) > 0:\n",
    "                    row_data['mean_twin_distance'] = np.mean(twin_distances)\n",
    "                    row_data['std_twin_distance'] = np.std(twin_distances)\n",
    "                \n",
    "                if len(non_twin_distances) > 0:\n",
    "                    row_data['mean_non_twin_distance'] = np.mean(non_twin_distances)\n",
    "                    row_data['std_non_twin_distance'] = np.std(non_twin_distances)\n",
    "                \n",
    "                # Distance separation metric (higher is better)\n",
    "                if len(twin_distances) > 0 and len(non_twin_distances) > 0:\n",
    "                    separation = abs(np.mean(non_twin_distances) - np.mean(twin_distances))\n",
    "                    row_data['distance_separation'] = separation\n",
    "        \n",
    "        # Add prediction statistics if available\n",
    "        if 'all_predictions' in eval_results and 'all_labels' in eval_results:\n",
    "            predictions = eval_results['all_predictions']\n",
    "            labels = eval_results['all_labels']\n",
    "            \n",
    "            # True/False positives and negatives\n",
    "            tp = np.sum((predictions == 1) & (labels == 1))\n",
    "            fp = np.sum((predictions == 1) & (labels == 0))\n",
    "            tn = np.sum((predictions == 0) & (labels == 0))\n",
    "            fn = np.sum((predictions == 0) & (labels == 1))\n",
    "            \n",
    "            row_data['true_positives'] = tp\n",
    "            row_data['false_positives'] = fp\n",
    "            row_data['true_negatives'] = tn\n",
    "            row_data['false_negatives'] = fn\n",
    "            \n",
    "            # Specificity (True Negative Rate)\n",
    "            if (tn + fp) > 0:\n",
    "                row_data['specificity'] = tn / (tn + fp)\n",
    "            else:\n",
    "                row_data['specificity'] = None\n",
    "        \n",
    "        rows_data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows_data, index=experiment_names)\n",
    "    \n",
    "    # Round numeric columns for better readability\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].round(4)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def compare_evaluation_results(df, sort_by='f1', ascending=False):\n",
    "    \"\"\"\n",
    "    Enhanced comparison function to analyze the combined evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame returned by combine_evaluation_results()\n",
    "        sort_by: Column name to sort by (default: 'f1')\n",
    "        ascending: Sort order (default: False for descending)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Sorted DataFrame with additional analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    if sort_by not in df.columns:\n",
    "        available_cols = list(df.columns)\n",
    "        raise ValueError(f\"Column '{sort_by}' not found. Available columns: {available_cols}\")\n",
    "    \n",
    "    # Sort the dataframe\n",
    "    df_sorted = df.sort_values(by=sort_by, ascending=ascending)\n",
    "    \n",
    "    print(f\"Evaluation Results Comparison (sorted by {sort_by}):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Display key metrics\n",
    "    key_metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'threshold']\n",
    "    available_key_metrics = [col for col in key_metrics if col in df.columns]\n",
    "    \n",
    "    if available_key_metrics:\n",
    "        print(\"\\nKey Performance Metrics:\")\n",
    "        print(df_sorted[available_key_metrics].to_string())\n",
    "    \n",
    "    # Show distance statistics if available\n",
    "    distance_cols = [col for col in df.columns if 'distance' in col.lower()]\n",
    "    if distance_cols:\n",
    "        print(f\"\\nDistance Statistics:\")\n",
    "        print(df_sorted[distance_cols].to_string())\n",
    "    \n",
    "    # Show confusion matrix components if available\n",
    "    confusion_cols = ['true_positives', 'false_positives', 'true_negatives', 'false_negatives']\n",
    "    available_confusion_cols = [col for col in confusion_cols if col in df.columns]\n",
    "    \n",
    "    if available_confusion_cols:\n",
    "        print(f\"\\nConfusion Matrix Components:\")\n",
    "        print(df_sorted[available_confusion_cols].to_string())\n",
    "    \n",
    "    # Find best performing experiment\n",
    "    if not df_sorted.empty:\n",
    "        best_experiment = df_sorted.index[0]\n",
    "        best_value = df_sorted.iloc[0][sort_by]\n",
    "        print(f\"\\nBest performing experiment: '{best_experiment}' with {sort_by} = {best_value:.4f}\")\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "# Combine results\n",
    "comparison_df = combine_evaluation_results(\n",
    "    baseline=model_metrics,\n",
    "    dist_gap=model_dist_gap_metrics,\n",
    "    separability=model_separability_metrics,\n",
    "    val_loss=model_val_loss_metrics,\n",
    "    auc=model_auc_metrics,\n",
    "    f1=model_f1_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_threshold(comparison_df.loc['baseline', 'threshold'])\n",
    "model_dist_gap.set_threshold(comparison_df.loc['dist_gap', 'threshold'])\n",
    "model_separability.set_threshold(comparison_df.loc['separability', 'threshold'])\n",
    "model_val_loss.set_threshold(comparison_df.loc['val_loss', 'threshold'])\n",
    "model_auc.set_threshold(comparison_df.loc['auc', 'threshold'])\n",
    "model_f1.set_threshold(comparison_df.loc['f1', 'threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'threshold': model.threshold,\n",
    "}, f\"{MODEL}/model.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_dist_gap.state_dict(),\n",
    "    'threshold': model_dist_gap.threshold,\n",
    "}, f\"{MODEL}/model_dist_gap.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_separability.state_dict(),\n",
    "    'threshold': model_separability.threshold,\n",
    "}, f\"{MODEL}/model_separability.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_val_loss.state_dict(),\n",
    "    'threshold': model_val_loss.threshold,\n",
    "}, f\"{MODEL}/model_val_loss.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_auc.state_dict(),\n",
    "    'threshold': model_auc.threshold,\n",
    "}, f\"{MODEL}/model_auc.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_f1.state_dict(),\n",
    "    'threshold': model_f1.threshold,\n",
    "}, f\"{MODEL}/model_f1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mkenics5200",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
